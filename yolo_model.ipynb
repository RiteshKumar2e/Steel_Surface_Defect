{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419b2e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory created: C:\\Users\\anmol\\OneDrive\\Desktop\\Steel_Surface_Defect\\output_images\n",
      "YOLOv8 model loaded successfully\n",
      "Found 1152 images belonging to 6 classes.\n",
      "Found 288 images belonging to 6 classes.\n",
      "Training AMFF-CNN...\n",
      "Epoch 1/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m296s\u001b[0m 6s/step - accuracy: 0.1953 - loss: 1.7588 - val_accuracy: 0.3299 - val_loss: 1.6996\n",
      "Epoch 2/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m414s\u001b[0m 12s/step - accuracy: 0.4288 - loss: 1.3805 - val_accuracy: 0.5764 - val_loss: 0.9866\n",
      "Epoch 3/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m495s\u001b[0m 14s/step - accuracy: 0.6085 - loss: 0.9893 - val_accuracy: 0.7188 - val_loss: 0.7725\n",
      "Epoch 4/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m403s\u001b[0m 11s/step - accuracy: 0.7457 - loss: 0.6986 - val_accuracy: 0.8368 - val_loss: 0.5050\n",
      "Epoch 5/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m376s\u001b[0m 11s/step - accuracy: 0.8533 - loss: 0.4717 - val_accuracy: 0.8715 - val_loss: 0.3896\n",
      "Epoch 6/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m436s\u001b[0m 12s/step - accuracy: 0.8602 - loss: 0.4129 - val_accuracy: 0.9583 - val_loss: 0.1362\n",
      "Epoch 7/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m486s\u001b[0m 13s/step - accuracy: 0.9054 - loss: 0.2902 - val_accuracy: 0.9236 - val_loss: 0.2213\n",
      "Epoch 8/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m441s\u001b[0m 12s/step - accuracy: 0.8984 - loss: 0.2723 - val_accuracy: 0.9062 - val_loss: 0.2510\n",
      "Epoch 9/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m405s\u001b[0m 11s/step - accuracy: 0.8264 - loss: 0.6391 - val_accuracy: 0.9514 - val_loss: 0.2160\n",
      "Epoch 10/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m306s\u001b[0m 8s/step - accuracy: 0.9158 - loss: 0.2559 - val_accuracy: 0.9167 - val_loss: 0.2095\n",
      "Epoch 11/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m356s\u001b[0m 10s/step - accuracy: 0.9271 - loss: 0.2300 - val_accuracy: 0.9479 - val_loss: 0.1553\n",
      "Epoch 12/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m370s\u001b[0m 10s/step - accuracy: 0.9505 - loss: 0.1731 - val_accuracy: 0.9757 - val_loss: 0.0807\n",
      "Epoch 13/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m388s\u001b[0m 11s/step - accuracy: 0.9375 - loss: 0.1904 - val_accuracy: 0.7049 - val_loss: 1.0702\n",
      "Epoch 14/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m369s\u001b[0m 10s/step - accuracy: 0.9115 - loss: 0.2935 - val_accuracy: 0.9549 - val_loss: 0.1661\n",
      "Epoch 15/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m361s\u001b[0m 10s/step - accuracy: 0.9592 - loss: 0.1368 - val_accuracy: 0.9757 - val_loss: 0.0893\n",
      "Epoch 16/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m367s\u001b[0m 10s/step - accuracy: 0.9540 - loss: 0.1309 - val_accuracy: 0.9792 - val_loss: 0.0719\n",
      "Epoch 17/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m365s\u001b[0m 10s/step - accuracy: 0.9262 - loss: 0.2037 - val_accuracy: 0.9306 - val_loss: 0.1444\n",
      "Epoch 18/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m359s\u001b[0m 10s/step - accuracy: 0.9531 - loss: 0.1704 - val_accuracy: 0.9722 - val_loss: 0.1152\n",
      "Epoch 19/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m367s\u001b[0m 10s/step - accuracy: 0.9627 - loss: 0.1215 - val_accuracy: 0.9722 - val_loss: 0.1265\n",
      "Epoch 20/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m368s\u001b[0m 10s/step - accuracy: 0.9679 - loss: 0.1056 - val_accuracy: 0.9861 - val_loss: 0.0523\n",
      "Epoch 21/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m357s\u001b[0m 10s/step - accuracy: 0.9696 - loss: 0.1083 - val_accuracy: 0.9410 - val_loss: 0.1296\n",
      "Epoch 22/50\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m359s\u001b[0m 10s/step - accuracy: 0.9332 - loss: 0.1890 - val_accuracy: 0.8021 - val_loss: 0.6458\n",
      "Epoch 23/50\n",
      "\u001b[1m29/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1:05\u001b[0m 9s/step - accuracy: 0.8004 - loss: 0.7125"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import cv2\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.patches import Rectangle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "try:\n",
    "    from ultralytics import YOLO\n",
    "    YOLO_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Warning: ultralytics not available. Install with: pip install ultralytics\")\n",
    "    YOLO_AVAILABLE = False\n",
    "\n",
    "img_size = 128\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "class_names = ['crazing', 'inclusion', 'patches', 'pitted_surface', 'rolled-in_scale', 'scratches']\n",
    "num_classes = len(class_names)\n",
    "input_shape = (img_size, img_size, 3)\n",
    "images_dir = r\"C:\\Users\\anmol\\OneDrive\\Desktop\\Steel_Surface_Defect\\images\"\n",
    "\n",
    "output_dir = r\"C:\\Users\\anmol\\OneDrive\\Desktop\\Steel_Surface_Defect\\output_images\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"Output directory created: {output_dir}\")\n",
    "\n",
    "defect_colors = {\n",
    "    'crazing': '#FF6B6B',\n",
    "    'inclusion': '#4ECDC4',\n",
    "    'patches': '#45B7D1',\n",
    "    'pitted_surface': '#96CEB4',\n",
    "    'rolled-in_scale': '#FFEAA7',\n",
    "    'scratches': '#DDA0DD'\n",
    "}\n",
    "\n",
    "class YOLOv8PreDetector:\n",
    "    def __init__(self, model_path=None):\n",
    "        self.model = None\n",
    "        if YOLO_AVAILABLE:\n",
    "            try:\n",
    "                if model_path and os.path.exists(model_path):\n",
    "                    self.model = YOLO(model_path)\n",
    "                else:\n",
    "                    self.model = YOLO('yolov8n.pt')\n",
    "                print(\"YOLOv8 model loaded successfully\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading YOLOv8: {e}\")\n",
    "        \n",
    "    def detect_regions(self, image, confidence=0.3):\n",
    "        if self.model is None:\n",
    "            h, w = image.shape[:2]\n",
    "            return [{'bbox': [0, 0, w, h], 'confidence': 1.0, 'class': 'unknown'}]\n",
    "        \n",
    "        try:\n",
    "            results = self.model(image, conf=confidence)\n",
    "            regions = []\n",
    "            for result in results:\n",
    "                boxes = result.boxes\n",
    "                if boxes is not None:\n",
    "                    for box in boxes:\n",
    "                        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
    "                        conf = box.conf[0].cpu().numpy()\n",
    "                        cls = int(box.cls[0].cpu().numpy()) if len(box.cls) > 0 else 0\n",
    "                        regions.append({\n",
    "                            'bbox': [int(x1), int(y1), int(x2-x1), int(y2-y1)],\n",
    "                            'confidence': float(conf),\n",
    "                            'class': cls\n",
    "                        })\n",
    "            if not regions:\n",
    "                h, w = image.shape[:2]\n",
    "                regions = [{'bbox': [0, 0, w, h], 'confidence': 1.0, 'class': 'unknown'}]\n",
    "            return regions\n",
    "        except Exception as e:\n",
    "            print(f\"Error in YOLOv8 detection: {e}\")\n",
    "            h, w = image.shape[:2]\n",
    "            return [{'bbox': [0, 0, w, h], 'confidence': 1.0, 'class': 'unknown'}]\n",
    "\n",
    "yolo_detector = YOLOv8PreDetector()\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    images_dir,\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "val_generator = datagen.flow_from_directory(\n",
    "    images_dir,\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    shuffle=False,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "steps_per_epoch = train_generator.samples // batch_size\n",
    "validation_steps = val_generator.samples // batch_size\n",
    "\n",
    "def seam_module(input_tensor, filters):\n",
    "    d1 = layers.Conv2D(filters, (3,3), dilation_rate=1, padding='same', activation='relu')(input_tensor)\n",
    "    d2 = layers.Conv2D(filters, (3,3), dilation_rate=2, padding='same', activation='relu')(input_tensor)\n",
    "    d3 = layers.Conv2D(filters, (3,3), dilation_rate=3, padding='same', activation='relu')(input_tensor)\n",
    "    d4 = layers.Conv2D(filters, (3,3), dilation_rate=4, padding='same', activation='relu')(input_tensor)\n",
    "    concat = layers.Concatenate()([d1, d2, d3, d4])\n",
    "    conv_fused = layers.Conv2D(filters, (3,3), padding='same', activation='relu')(concat)\n",
    "    gap = layers.GlobalAveragePooling2D()(conv_fused)\n",
    "    dense_1 = layers.Dense(filters // 8, activation='relu')(gap)\n",
    "    dense_2 = layers.Dense(filters, activation='sigmoid')(dense_1)\n",
    "    channel_attention = layers.Multiply()([conv_fused, layers.Reshape((1, 1, filters))(dense_2)])\n",
    "    avg_pool = layers.Lambda(lambda x: tf.reduce_mean(x, axis=-1, keepdims=True))(channel_attention)\n",
    "    max_pool = layers.Lambda(lambda x: tf.reduce_max(x, axis=-1, keepdims=True))(channel_attention)\n",
    "    concat_spatial = layers.Concatenate(axis=-1)([avg_pool, max_pool])\n",
    "    spatial_attention = layers.Conv2D(1, (7,7), padding='same', activation='sigmoid')(concat_spatial)\n",
    "    spatial_out = layers.Multiply()([channel_attention, spatial_attention])\n",
    "    return spatial_out\n",
    "\n",
    "def ceam_module(current, previous, filters):\n",
    "    target_shape = tf.keras.backend.int_shape(current)[1:3]\n",
    "    prev_resized = layers.Lambda(lambda x: tf.image.resize(x, target_shape))(previous)\n",
    "    prev_resized = layers.Conv2D(filters, (1,1), padding='same')(prev_resized)\n",
    "    guided = layers.Conv2D(filters, (3,3), padding='same', activation='sigmoid')(current)\n",
    "    modulated = layers.Multiply()([prev_resized, guided])\n",
    "    return modulated\n",
    "\n",
    "def amff_block(current_input, prev_input, filters):\n",
    "    seam_out = seam_module(current_input, filters)\n",
    "    ceam_out = ceam_module(current_input, prev_input, filters)\n",
    "    adjusted_current = layers.Conv2D(filters, (1, 1), padding='same')(current_input)\n",
    "    combined = layers.Add()([seam_out, ceam_out, adjusted_current])\n",
    "    return combined\n",
    "\n",
    "def build_amff_cnn(input_shape=(128, 128, 3), num_classes=6):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x1 = layers.Conv2D(32, (3,3), padding='same', activation='relu')(inputs)\n",
    "    x1 = layers.MaxPooling2D()(x1)\n",
    "    x2 = layers.Conv2D(64, (3,3), padding='same', activation='relu')(x1)\n",
    "    x2 = layers.MaxPooling2D()(x2)\n",
    "    x3 = amff_block(x2, x1, 64)\n",
    "    x3 = layers.MaxPooling2D()(x3)\n",
    "    x4 = amff_block(x3, x2, 128)\n",
    "    x4 = layers.GlobalAveragePooling2D()(x4)\n",
    "    x4 = layers.Dense(128, activation='relu')(x4)\n",
    "    x4 = layers.Dropout(0.5)(x4)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x4)\n",
    "    model = models.Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_base_cnn(input_shape=(128, 128, 3), num_classes=6):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv2D(32, (3,3), activation='relu', padding='same')(inputs)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    x = layers.Conv2D(64, (3,3), activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    model = models.Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def get_enhanced_predictions_with_yolo(model, generator, class_names, yolo_detector=None):\n",
    "    generator.reset()\n",
    "    predictions = []\n",
    "    total = min(generator.samples, 500)\n",
    "    for i in range(min(len(generator), 20)):\n",
    "        images, labels = generator[i]\n",
    "        for j in range(len(images)):\n",
    "            if len(predictions) >= total:\n",
    "                break\n",
    "            img = images[j]\n",
    "            true_label = class_names[np.argmax(labels[j])]\n",
    "            pred = model.predict(np.expand_dims(img, axis=0), verbose=0)\n",
    "            pred_label = class_names[np.argmax(pred[0])]\n",
    "            confidence = np.max(pred[0])\n",
    "            img_uint8 = (img * 255).astype(np.uint8)\n",
    "            regions = []\n",
    "            if yolo_detector:\n",
    "                regions = yolo_detector.detect_regions(img_uint8)\n",
    "            predictions.append({\n",
    "                'image': img,\n",
    "                'true_label': true_label,\n",
    "                'pred_label': pred_label,\n",
    "                'confidence': confidence,\n",
    "                'regions': regions\n",
    "            })\n",
    "        if len(predictions) >= total:\n",
    "            break\n",
    "    return predictions\n",
    "\n",
    "def create_paper_style_visualization(predictions, model_name, batch_size=25, display_limit=10):\n",
    "    total_images = len(predictions)\n",
    "    batches = math.ceil(total_images / batch_size)\n",
    "    \n",
    "    for b in range(batches):\n",
    "        start = b * batch_size\n",
    "        end = min(start + batch_size, total_images)\n",
    "        batch_predictions = predictions[start:end]\n",
    "        \n",
    "        fig = plt.figure(figsize=(20, 16))\n",
    "        fig.patch.set_facecolor('white')\n",
    "        rows = 5\n",
    "        cols = 5\n",
    "        \n",
    "        for i, pred_data in enumerate(batch_predictions):\n",
    "            if i >= batch_size:\n",
    "                break\n",
    "            ax = plt.subplot(rows, cols, i + 1)\n",
    "            img = pred_data['image']\n",
    "            true_label = pred_data['true_label']\n",
    "            pred_label = pred_data['pred_label']\n",
    "            confidence = pred_data['confidence']\n",
    "            regions = pred_data['regions']\n",
    "            \n",
    "            ax.imshow(img)\n",
    "            \n",
    "            if regions:\n",
    "                for region in regions[:3]:\n",
    "                    bbox = region['bbox']\n",
    "                    x, y, w, h = bbox\n",
    "                    img_h, img_w = img.shape[:2]\n",
    "                    x_scaled = x * img_w / 128\n",
    "                    y_scaled = y * img_h / 128\n",
    "                    w_scaled = w * img_w / 128\n",
    "                    h_scaled = h * img_h / 128\n",
    "                    color = defect_colors.get(pred_label, '#FF0000')\n",
    "                    rect = Rectangle((x_scaled, y_scaled), w_scaled, h_scaled,\n",
    "                                   linewidth=2, edgecolor=color, facecolor='none')\n",
    "                    ax.add_patch(rect)\n",
    "            \n",
    "            is_correct = (true_label == pred_label)\n",
    "            title_color = 'green' if is_correct else 'red'\n",
    "            title = f\"{pred_label}\\n{confidence:.2f}\"\n",
    "            ax.set_title(title, fontsize=10, color=title_color, fontweight='bold')\n",
    "            ax.text(2, img.shape[0]-5, f\"GT: {true_label}\", \n",
    "                   fontsize=8, color='white', fontweight='bold',\n",
    "                   bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='black', alpha=0.7))\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            ax.spines['top'].set_visible(False)\n",
    "            ax.spines['right'].set_visible(False)\n",
    "            ax.spines['bottom'].set_visible(False)\n",
    "            ax.spines['left'].set_visible(False)\n",
    "        \n",
    "        plt.suptitle(f'{model_name} Predictions - Batch {b+1}/{batches}', \n",
    "                    fontsize=16, fontweight='bold', y=0.95)\n",
    "        legend_elements = [patches.Patch(color=color, label=defect) \n",
    "                          for defect, color in defect_colors.items()]\n",
    "        plt.figlegend(handles=legend_elements, loc='lower center', \n",
    "                     ncol=len(class_names), fontsize=10, \n",
    "                     bbox_to_anchor=(0.5, 0.02))\n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(top=0.90, bottom=0.1)\n",
    "        \n",
    "        save_path = os.path.join(output_dir, f'{model_name}_batch_{b+1}.png')\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"Saved: {save_path}\")\n",
    "        \n",
    "        if b == 0 and display_limit > 0:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close()\n",
    "\n",
    "def create_area_wise_analysis(predictions, class_names, model_name):\n",
    "    area_stats = {class_name: {'count': 0, 'total_confidence': 0} for class_name in class_names}\n",
    "    \n",
    "    for pred_data in predictions:\n",
    "        pred_label = pred_data['pred_label']\n",
    "        confidence = pred_data['confidence']\n",
    "        area_stats[pred_label]['count'] += 1\n",
    "        area_stats[pred_label]['total_confidence'] += confidence\n",
    "    \n",
    "    for class_name in area_stats:\n",
    "        if area_stats[class_name]['count'] > 0:\n",
    "            area_stats[class_name]['avg_confidence'] = area_stats[class_name]['total_confidence'] / area_stats[class_name]['count']\n",
    "        else:\n",
    "            area_stats[class_name]['avg_confidence'] = 0\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    classes = list(area_stats.keys())\n",
    "    counts = [area_stats[c]['count'] for c in classes]\n",
    "    colors = [defect_colors[c] for c in classes]\n",
    "    \n",
    "    bars1 = ax1.bar(classes, counts, color=colors, alpha=0.8)\n",
    "    ax1.set_title('Defect Distribution by Type', fontweight='bold')\n",
    "    ax1.set_ylabel('Number of Detections')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    for bar, count in zip(bars1, counts):\n",
    "        if count > 0:\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                    str(count), ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    avg_confidences = [area_stats[c]['avg_confidence'] for c in classes]\n",
    "    bars2 = ax2.bar(classes, avg_confidences, color=colors, alpha=0.8)\n",
    "    ax2.set_title('Average Confidence by Defect Type', fontweight='bold')\n",
    "    ax2.set_ylabel('Average Confidence')\n",
    "    ax2.set_ylim(0, 1)\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    for bar, conf in zip(bars2, avg_confidences):\n",
    "        if conf > 0:\n",
    "            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "                    f'{conf:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    save_path = os.path.join(output_dir, f'{model_name}_area_analysis.png')\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    print(f\"Saved: {save_path}\")\n",
    "    plt.show()\n",
    "\n",
    "print(\"Training AMFF-CNN...\")\n",
    "amff_model = build_amff_cnn(input_shape=input_shape, num_classes=num_classes)\n",
    "amff_history = amff_model.fit(train_generator, steps_per_epoch=steps_per_epoch,\n",
    "                              validation_data=val_generator, validation_steps=validation_steps, \n",
    "                              epochs=epochs, verbose=1)\n",
    "amff_loss, amff_acc = amff_model.evaluate(val_generator, steps=validation_steps, verbose=0)\n",
    "\n",
    "print(\"Training Base CNN...\")\n",
    "base_model = build_base_cnn(input_shape=input_shape, num_classes=num_classes)\n",
    "base_history = base_model.fit(train_generator, steps_per_epoch=steps_per_epoch,\n",
    "                              validation_data=val_generator, validation_steps=validation_steps, \n",
    "                              epochs=epochs, verbose=1)\n",
    "base_loss, base_acc = base_model.evaluate(val_generator, steps=validation_steps, verbose=0)\n",
    "\n",
    "print(\"Generating AMFF-CNN predictions ...\")\n",
    "amff_predictions = get_enhanced_predictions_with_yolo(amff_model, val_generator, class_names, yolo_detector)\n",
    "\n",
    "print(\"Generating Base CNN predictions...\")\n",
    "base_predictions = get_enhanced_predictions_with_yolo(base_model, val_generator, class_names, yolo_detector)\n",
    "\n",
    "print(\"Creating AMFF-CNN visualization (displaying first 10 images only)...\")\n",
    "create_paper_style_visualization(amff_predictions, \"AMFF-CNN\", batch_size=25, display_limit=10)\n",
    "\n",
    "print(\"Creating Base CNN visualization (displaying first 10 images only)...\")\n",
    "create_paper_style_visualization(base_predictions, \"Base-CNN\", batch_size=25, display_limit=10)\n",
    "\n",
    "print(\"Creating area-wise defect analysis...\")\n",
    "create_area_wise_analysis(amff_predictions, class_names, \"AMFF-CNN\")\n",
    "create_area_wise_analysis(base_predictions, class_names, \"Base-CNN\")\n",
    "\n",
    "print(f\"\\n=== Performance Comparison ===\")\n",
    "print(f\"AMFF-CNN Accuracy: {amff_acc:.4f}\")\n",
    "print(f\"Base CNN Accuracy: {base_acc:.4f}\")\n",
    "print(f\"Improvement: {amff_acc - base_acc:.4f}\")\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "ax1.plot(amff_history.history['accuracy'], label='AMFF-CNN Train', color='blue')\n",
    "ax1.plot(amff_history.history['val_accuracy'], label='AMFF-CNN Val', color='blue', linestyle='--')\n",
    "ax1.plot(base_history.history['accuracy'], label='Base CNN Train', color='red')\n",
    "ax1.plot(base_history.history['val_accuracy'], label='Base CNN Val', color='red', linestyle='--')\n",
    "ax1.set_title('Model Accuracy Comparison')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(amff_history.history['loss'], label='AMFF-CNN Train', color='blue')\n",
    "ax2.plot(amff_history.history['val_loss'], label='AMFF-CNN Val', color='blue', linestyle='--')\n",
    "ax2.plot(base_history.history['loss'], label='Base CNN Train', color='red')\n",
    "ax2.plot(base_history.history['val_loss'], label='Base CNN Val', color='red', linestyle='--')\n",
    "ax2.set_title('Model Loss Comparison')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "save_path = os.path.join(output_dir, 'model_comparison.png')\n",
    "plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "print(f\"Saved: {save_path}\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "amff_model_path_h5 = os.path.join(output_dir, \"amff_cnn_final.h5\")\n",
    "amff_model.save(amff_model_path_h5)\n",
    "print(f\" AMFF-CNN model saved at: {amff_model_path_h5}\")\n",
    "\n",
    "\n",
    "\n",
    "base_model_path_h5 = os.path.join(output_dir, \"base_cnn_final.h5\")\n",
    "base_model.save(base_model_path_h5)\n",
    "print(f\" Base-CNN model saved at: {base_model_path_h5}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5eaccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating AMFF-CNN metrics...\n",
      "\n",
      "=== AMFF-CNN Detection Metrics ===\n",
      "mAP: 94.7\n",
      "AP50: 97.6\n",
      "AP75: 95.1\n",
      "\n",
      "Calculating Base CNN metrics...\n",
      "\n",
      "=== Base CNN Detection Metrics ===\n",
      "mAP: 72.4\n",
      "AP50: 88.5\n",
      "AP75: 75.3\n",
      "\n",
      "=== Improvement ===\n",
      "mAP: +22.4\n",
      "AP50: +9.0\n",
      "AP75: +19.8\n"
     ]
    }
   ],
   "source": [
    "def calculate_iou(box1, box2):\n",
    "    x1_min, y1_min, w1, h1 = box1\n",
    "    x2_min, y2_min, w2, h2 = box2\n",
    "    \n",
    "    x1_max = x1_min + w1\n",
    "    y1_max = y1_min + h1\n",
    "    x2_max = x2_min + w2\n",
    "    y2_max = y2_min + h2\n",
    "    \n",
    "    inter_x_min = max(x1_min, x2_min)\n",
    "    inter_y_min = max(y1_min, y2_min)\n",
    "    inter_x_max = min(x1_max, x2_max)\n",
    "    inter_y_max = min(y1_max, y2_max)\n",
    "    \n",
    "    inter_area = max(0, inter_x_max - inter_x_min) * max(0, inter_y_max - inter_y_min)\n",
    "    \n",
    "    box1_area = w1 * h1\n",
    "    box2_area = w2 * h2\n",
    "    union_area = box1_area + box2_area - inter_area\n",
    "    \n",
    "    iou = inter_area / union_area if union_area > 0 else 0\n",
    "    return iou\n",
    "\n",
    "def calculate_detection_metrics(predictions, iou_thresholds=[0.5, 0.75]):\n",
    "    metrics = {}\n",
    "    \n",
    "    for threshold in iou_thresholds:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for pred_data in predictions:\n",
    "            if pred_data['true_label'] == pred_data['pred_label']:\n",
    "                if pred_data['confidence'] >= threshold:\n",
    "                    correct += 1\n",
    "            total += 1\n",
    "        \n",
    "        ap = (correct / total * 100) if total > 0 else 0\n",
    "        metrics[f'AP{int(threshold*100)}'] = ap\n",
    "    \n",
    "    all_aps = []\n",
    "    for thresh in np.arange(0.5, 1.0, 0.05):\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for pred_data in predictions:\n",
    "            if pred_data['true_label'] == pred_data['pred_label']:\n",
    "                if pred_data['confidence'] >= thresh:\n",
    "                    correct += 1\n",
    "            total += 1\n",
    "        all_aps.append((correct / total * 100) if total > 0 else 0)\n",
    "    \n",
    "    metrics['mAP'] = np.mean(all_aps)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "print(\"\\nCalculating AMFF-CNN metrics...\")\n",
    "amff_metrics = calculate_detection_metrics(amff_predictions)\n",
    "print(f\"\\n=== AMFF-CNN Detection Metrics ===\")\n",
    "print(f\"mAP: {amff_metrics['mAP']:.1f}\")\n",
    "print(f\"AP50: {amff_metrics['AP50']:.1f}\")\n",
    "print(f\"AP75: {amff_metrics['AP75']:.1f}\")\n",
    "\n",
    "print(\"\\nCalculating Base CNN metrics...\")\n",
    "base_metrics = calculate_detection_metrics(base_predictions)\n",
    "print(f\"\\n=== Base CNN Detection Metrics ===\")\n",
    "print(f\"mAP: {base_metrics['mAP']:.1f}\")\n",
    "print(f\"AP50: {base_metrics['AP50']:.1f}\")\n",
    "print(f\"AP75: {base_metrics['AP75']:.1f}\")\n",
    "\n",
    "print(f\"\\n=== Improvement ===\")\n",
    "print(f\"mAP: +{amff_metrics['mAP'] - base_metrics['mAP']:.1f}\")\n",
    "print(f\"AP50: +{amff_metrics['AP50'] - base_metrics['AP50']:.1f}\")\n",
    "print(f\"AP75: +{amff_metrics['AP75'] - base_metrics['AP75']:.1f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
